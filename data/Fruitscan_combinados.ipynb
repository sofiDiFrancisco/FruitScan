{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo FruitScan - Comparación de ResNet34, ResNet36 y VGG19\n",
    "\n",
    "Este notebook entrena y compara tres modelos de **Redes Neuronales Convolucionales (CNN)** utilizando un dataset comprimido en formato ZIP. El objetivo principal es evaluar el rendimiento de **ResNet18**, **ResNet34**, y **VGG19** en la clasificación de frutas frescas y podridas.\n",
    "\n",
    "## Contenido:\n",
    "- **Dataset:** Imágenes de frutas frescas y podridas (manzanas, bananas, naranjas).\n",
    "- **Modelos:** ResNet18, ResNet34 y VGG19 (pre-entrenados y fine-tuned).\n",
    "- **Evaluación:** Curvas de pérdida, precisión por época y matrices de confusión para cada modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## Paso 1: Descarga y Extracción del Dataset\n",
    "\n",
    "En este paso, se descarga el dataset de frutas directamente desde Google Drive y se extrae su contenido en el directorio `data/`. Esto asegura que las imágenes estén disponibles para el entrenamiento y la validación de los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Descargar directamente el ZIP desde Google Drive (sin necesidad de montar cuenta)\n",
    "file_id = \"1WEHp4vAUuu1wswR6tQSkxgC2shIpFIJS\"\n",
    "output = \"fruits_dataset.zip\"\n",
    "gdown.download(id=file_id, output=output, quiet=False)\n",
    "\n",
    "# Extraer contenido\n",
    "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data/\")\n",
    "\n",
    "# Confirmar clases\n",
    "extract_path = \"data/dataset/dataset/train\"\n",
    "print(\"✅ Clases detectadas:\", os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 2: Importación de Librerías\n",
    "\n",
    "Se importan todas las librerías necesarias para la manipulación de datos, la construcción y entrenamiento de modelos de PyTorch, y la visualización de resultados. Esto incluye `torch`, `torchvision`, `matplotlib`, y `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 3: Configuraciones Iniciales y Preparación del Dataset\n",
    "\n",
    "Aquí se definen los parámetros generales como el tamaño del batch, las dimensiones de las imágenes y el número de épocas. Además, se configuran las **transformaciones de datos** necesarias (redimensionamiento, conversión a tensor y normalización) y se cargan los datasets de entrenamiento y prueba utilizando `ImageFolder` de `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros generales\n",
    "batch_size = 8\n",
    "img_size = 224\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = \"data/dataset/dataset\"\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset usando carpetas train/ y test/ directamente\n",
    "train_dir = \"data/dataset/dataset/train\"\n",
    "test_dir = \"data/dataset/dataset/test\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(\"Clases detectadas:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 4: Función de Entrenamiento Genérica\n",
    "\n",
    "Se define una función `entrenar_modelo` que encapsula el bucle de entrenamiento y validación. Esta función es genérica y puede ser utilizada con cualquiera de los modelos de CNN. Calcula y retorna las pérdidas de entrenamiento y validación, las precisiones de validación, y las predicciones reales para la generación de la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(model, train_loader, val_loader, optimizer, criterion, epochs, nombre):\n",
    "    model = model.to(device)\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"\\u2705 Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 5: Entrenamiento de Modelos\n",
    "\n",
    "Aquí se instancian, modifican (para adaptar la capa de salida al número de clases) y entrenan los tres modelos pre-entrenados: **ResNet18**, **ResNet34**, y **VGG19**. Para cada modelo, se congela la mayoría de las capas pre-entrenadas y solo se entrena la capa de clasificación final. Se utiliza el optimizador Adam y la función de pérdida CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "print(\"\\n--- Entrenando ResNet18 ---\")\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, len(class_names))\n",
    "\n",
    "optimizer_r18 = optim.Adam(resnet18.fc.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "resnet18, r18_train, r18_val, r18_acc, y_true_r18, y_pred_r18 = entrenar_modelo(\n",
    "    resnet18, train_loader, val_loader, optimizer_r18, criterion, epochs, \"ResNet18\"\n",
    ")\n",
    "\n",
    "# ResNet34\n",
    "print(\"\\n--- Entrenando ResNet34 ---\")\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "resnet34.fc = nn.Linear(resnet34.fc.in_features, len(class_names))\n",
    "\n",
    "optimizer_r34 = optim.Adam(resnet34.fc.parameters(), lr=0.001)\n",
    "\n",
    "resnet34, r34_train, r34_val, r34_acc, y_true_r34, y_pred_r34 = entrenar_modelo(\n",
    "    resnet34, train_loader, val_loader, optimizer_r34, criterion, epochs, \"ResNet34\"\n",
    ")\n",
    "\n",
    "# VGG19\n",
    "print(\"\\n--- Entrenando VGG19 ---\")\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "for param in vgg19.features.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, len(class_names))\n",
    "\n",
    "optimizer_vgg = optim.Adam(vgg19.classifier.parameters(), lr=0.001)\n",
    "\n",
    "vgg19, vgg_train, vgg_val, vgg_acc, y_true_vgg, y_pred_vgg = entrenar_modelo(\n",
    "    vgg19, train_loader, val_loader, optimizer_vgg, criterion, epochs, \"VGG19\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 6: Comparación de Métricas de los Modelos\n",
    "\n",
    "Se generan gráficos para comparar visualmente las curvas de pérdida de validación y la precisión de validación a lo largo de las épocas para cada uno de los modelos. Además, se imprime la precisión final (macro average) de cada modelo para una comparación directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(metric_dict, title, ylabel):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label, values in metric_dict.items():\n",
    "        plt.plot(values, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Diccionarios con los valores por modelo\n",
    "loss_val_dict = {\n",
    "    \"ResNet18\": r18_val,\n",
    "    \"ResNet34\": r34_val,\n",
    "    \"VGG19\": vgg_val\n",
    "}\n",
    "\n",
    "acc_val_dict = {\n",
    "    \"ResNet18\": r18_acc,\n",
    "    \"ResNet34\": r34_acc,\n",
    "    \"VGG19\": vgg_acc\n",
    "}\n",
    "\n",
    "# Cálculo de precisión por época no disponible directamente -> omitimos para no inventar valores\n",
    "# Se podría hacer con batches pero complica la función, por ahora mostramos precisión final por modelo\n",
    "def calcular_precision_final(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "precision_dict = {\n",
    "    \"ResNet18\": calcular_precision_final(y_true_r18, y_pred_r18),\n",
    "    \"ResNet34\": calcular_precision_final(y_true_r34, y_pred_r34),\n",
    "    \"VGG19\": calcular_precision_final(y_true_vgg, y_pred_vgg),\n",
    "}\n",
    "\n",
    "plot_metric_comparison(loss_val_dict, \"Validation Loss\", \"Loss\")\n",
    "plot_metric_comparison(acc_val_dict, \"Validation Accuracy\", \"Accuracy\")\n",
    "\n",
    "# Mostrar precisión final\n",
    "print(\"📌 Precision por modelo (macro avg):\")\n",
    "for modelo, prec in precision_dict.items():\n",
    "    print(f\"{modelo}: {prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 6.1: Análisis Profundo y Determinación del Mejor Modelo\n",
    "\n",
    "La fase de análisis es donde interpretamos los resultados numéricos y visuales para extraer conclusiones significativas sobre el rendimiento de cada modelo. Basándonos en los gráficos de curvas de pérdida de validación, la precisión de validación a lo largo de las épocas, y la precisión final (`macro avg`), podemos identificar el modelo que exhibe el rendimiento más robusto y generalizado para nuestro problema de clasificación de frutas frescas vs. podridas.\n",
    "\n",
    "Para realizar esta determinación, nos enfocamos en las siguientes métricas clave:\n",
    "\n",
    "1.  **Pérdida de Validación (`Validation Loss`)**: Este valor es un indicador crucial de cuán bien está aprendiendo el modelo y generalizando a datos no vistos. Un `Validation Loss` bajo y estable a lo largo de las épocas sugiere que el modelo no solo está aprendiendo de manera efectiva, sino que también evita el sobreajuste (`overfitting`). Una pérdida que disminuye continuamente en el entrenamiento pero aumenta en la validación es una señal clara de sobreajuste.\n",
    "\n",
    "2.  **Precisión de Validación (`Validation Accuracy`)**: Esta métrica nos dice la proporción de predicciones correctas que el modelo realiza en el conjunto de validación. Un valor más alto indica una mejor capacidad de clasificación. Es importante observar no solo el valor final, sino también la trayectoria: ¿la precisión se estabiliza, sigue mejorando o comienza a disminuir (posible sobreajuste)?\n",
    "\n",
    "3.  **Precisión Final (`Precision (macro avg)`)**: Como se explicó anteriormente, la precisión macro promedio es fundamental porque calcula la precisión para cada clase de forma independiente y luego promedia esos valores. Esto es especialmente valioso en datasets donde las clases pueden no estar balanceadas, ya que asegura que el rendimiento en clases minoritarias no sea ignorado por un buen rendimiento en clases mayoritarias. Una alta precisión macro promedio indica que el modelo es consistentemente bueno en todas las categorías de frutas.\n",
    "\n",
    "### **Observaciones Típicas y Fundamentos Arquitectónicos:**\n",
    "\n",
    "* **VGG19**: Esta arquitectura es conocida por su simplicidad (bloques de capas convolucionales y de *pooling* seguidos por capas densas) y su gran profundidad. Sin embargo, su enorme número de parámetros puede hacer que sea más lenta de entrenar y, en tareas de *transfer learning* donde solo se entrena la capa final, a veces puede no alcanzar la misma eficiencia o capacidad de adaptación que las arquitecturas con **conexiones residuales**. Su rendimiento puede ser bueno, pero a menudo se ve superado por las ResNets en datasets más complejos o con menos datos disponibles para el *fine-tuning* de la capa final.\n",
    "\n",
    "* **ResNet18**: Como una de las arquitecturas ResNet más ligeras, ResNet18 introduce las innovadoras **conexiones residuales (skip connections)**. Estas conexiones permiten que la información pase directamente de capas anteriores a capas posteriores, mitigando el problema del *vanishing gradient* (desaparición del gradiente) en redes muy profundas. Esto facilita el entrenamiento y permite a la red aprender características más complejas. ResNet18 generalmente ofrece un excelente equilibrio entre un rendimiento competitivo y una eficiencia computacional razonable, lo que la convierte en una opción muy popular para *transfer learning*.\n",
    "\n",
    "* **ResNet34**: Con más capas que ResNet18 (pero aún dentro de la misma familia de arquitecturas residuales), ResNet34 tiene una mayor capacidad de modelado. Esto le permite capturar patrones y características más intrincadas en las imágenes. En muchos casos, ResNet34 puede lograr una **precisión ligeramente superior** a ResNet18 y una **menor pérdida de validación**, aunque a costa de un tiempo de entrenamiento un poco mayor y un consumo de recursos computacionales marginalmente más alto. Su diseño intrínseco con bloques residuales le permite escalar bien la profundidad sin los problemas de optimización que enfrentan otras redes profundas como VGG sin estas conexiones.\n",
    "\n",
    "**Conclusión y Razón del Mejor Rendimiento (basada en resultados esperados y experiencia general):**\n",
    "\n",
    "Tras una revisión de los gráficos de *Validation Loss* y *Validation Accuracy*, así como la métrica numérica de *Precision (macro avg)*, se espera que el modelo **ResNet34** demuestre ser el de **mejor rendimiento** para esta tarea de clasificación de frutas.\n",
    "\n",
    "Las razones principales de su superioridad probable son:\n",
    "1.  **Profundidad Optimizada**: ResNet34 es lo suficientemente profunda como para aprender representaciones ricas y complejas de las imágenes, pero no tan profunda como para volverse intratable con el *transfer learning* en este dataset. Su arquitectura, con más bloques residuales que ResNet18, le confiere una mayor capacidad de aprendizaje.\n",
    "2.  **Eficacia de las Conexiones Residuales**: Las *skip connections* de la familia ResNet son clave. Permiten que los gradientes fluyan más fácilmente a través de la red, lo que evita la saturación de gradientes y facilita el entrenamiento de capas más profundas. Esto es crucial para aprender las sutiles diferencias entre frutas frescas y podridas, incluso con cambios en texturas, colores o formas.\n",
    "3.  **Generalización Mejorada**: Las ResNets, gracias a su diseño, tienden a generalizar mejor a datos no vistos, lo que se refleja en una menor pérdida de validación y una mayor precisión de validación en comparación con VGG19, que podría ser más propensa al sobreajuste o a una convergencia más lenta en este tipo de escenarios de *transfer learning*.\n",
    "\n",
    "En resumen, el **ResNet34** logra un equilibrio óptimo entre complejidad de modelo y la capacidad de aprender características discriminativas para esta tarea, superando típicamente a sus contrapartes en términos de precisión y robustez. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 7: Matrices de Confusión\n",
    "\n",
    "Se generan y muestran las matrices de confusión normalizadas para cada modelo. Estas matrices permiten visualizar el rendimiento de clasificación por clase, identificando dónde los modelos aciertan y dónde se equivocan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_confusion(y_true, y_pred, modelo):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "    plt.title(f\"Normalized Confusion Matrix - {modelo}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_confusion(y_true_r18, y_pred_r18, \"ResNet18\")\n",
    "mostrar_confusion(y_true_r34, y_pred_r34, \"ResNet34\")\n",
    "mostrar_confusion(y_true_vgg, y_pred_vgg, \"VGG19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 8: Guardado de Modelos\n",
    "\n",
    "Los pesos entrenados de cada modelo se guardan en archivos `.pth` separados. Esto permite recargar y reutilizar los modelos en el futuro sin necesidad de reentrenarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18.state_dict(), \"model_resnet18.pth\")\n",
    "torch.save(resnet34.state_dict(), \"model_resnet34.pth\")\n",
    "torch.save(vgg19.state_dict(), \"model_vgg19.pth\")\n",
    "\n",
    "print(\"✅ Modelos guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 9: Mostrar Información sobre la Clase Predicha (Simulación)\n",
    "\n",
    "Este paso simula cómo se podría usar la predicción de un modelo en una aplicación real, como una interfaz de usuario. Se proporciona información descriptiva para cada clase de fruta, demostrando cómo se podría integrar la salida del modelo con datos adicionales para el usuario final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_info = {\n",
    "    'freshapples': 'These are fresh apples. A good source of fiber and vitamin C.',\n",
    "    'freshbanana': 'These are fresh bananas. High in potassium.',\n",
    "    'freshoranges': 'These are fresh oranges. Rich in vitamin C and antioxidants.',\n",
    "    'rottenapples': 'These are rotten apples. Not suitable for consumption.',\n",
    "    'rottenbanana': 'These are rotten bananas. Sometimes usable for cooking, but handle with care.',\n",
    "    'rottenoranges': 'These are rotten oranges. Should be discarded.'\n",
    "}\n",
    "\n",
    "# Simulación de uso posterior (por ejemplo, en Streamlit)\n",
    "# Podrías reemplazar 'predicted_class_name' por la clase detectada en tiempo real\n",
    "predicted_class_name = \"freshapples\"\n",
    "\n",
    "if predicted_class_name in fruit_info:\n",
    "    print(f\"\\nℹ️ Info for predicted fruit ({predicted_class_name}):\")\n",
    "    print(fruit_info[predicted_class_name])\n",
    "else:\n",
    "    print(\"\\nℹ️ No info available for the predicted class.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
